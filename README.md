# ğŸ§  byteQgennie â€“ Local PDF + Ollama Chatbot (FAISS + LangChain)

`byteQgennie` is a local Retrieval-Augmented Generation (RAG) chatbot that:

- reads your **PDF files** from a folder,
- builds a **FAISS vector index** using **HuggingFace embeddings**,
- uses **Ollama (llama3.2)** as the LLM,
- serves a **Flask web endpoint** that you can call from a frontend or simple HTML chat UI.

It is designed to run **fully locally** (aside from model downloads), with automatic detection of new PDFs and periodic index updates.

---

## ğŸš€ What This Project Does (Current Capabilities)

Right now, this project is capable of:

- ğŸ“„ **Loading PDFs** from the `./data/` folder
- âœ‚ï¸ **Splitting text into chunks** using `RecursiveCharacterTextSplitter`
- ğŸ§¬ **Embedding text chunks** using `sentence-transformers/all-MiniLM-L6-v2` via `HuggingFaceEmbeddings`
- ğŸ“š **Building a FAISS index** for fast similarity search
- ğŸ’¾ **Saving and reusing precomputed data**:
  - `precomputed_data/index.faiss` â€“ FAISS index
  - `precomputed_data/docs.pkl` â€“ list of LangChain `Document` chunks
  - `precomputed_data/processed_files.pkl` â€“ names of PDFs already indexed
- ğŸ” **Detecting new PDFs automatically** on startup and in a **background thread** (periodic checks)
- ğŸ¤– **Answering user questions** using:
  1. FAISS to find the most relevant chunk  
  2. `OllamaLLM(model="llama3.2")` to generate a natural, refined answer
- ğŸ‘‹ Simple **greetings & farewells**:
  - Responds nicely to â€œhiâ€, â€œhelloâ€, â€œbyeâ€, etc.
- ğŸŒ Exposes endpoints:
  - `/` â€“ renders `index.html` template (simple chat UI)
  - `/get` â€“ returns the chatbot response for a `msg` query

---

## ğŸ§± Tech Stack

### Backend

- **Python**
- **Flask** â€“ web framework
- **FAISS** â€“ vector similarity search (via `faiss` + `langchain_community.vectorstores.FAISS`)
- **LangChain** â€“ for:
  - `PyPDFLoader` (PDF loading)
  - `RecursiveCharacterTextSplitter` (chunking)
  - `Document` type
  - `InMemoryDocstore`
- **HuggingFaceEmbeddings**
  - Model: `sentence-transformers/all-MiniLM-L6-v2`
- **Ollama LLM**
  - `OllamaLLM` from `langchain_ollama`
  - Model: `llama3.2`

### Supporting Libraries

- `numpy` â€“ for numeric arrays used by FAISS
- `pickle` â€“ for serializing docs + processed file names
- `threading`, `time`, `os` â€“ standard library for background tasks and file management

---

## ğŸ“ Folder & File Layout

Expected structure:

```text
byteQgennie/
â”œâ”€ app.py                       # (the file you shared)
â”œâ”€ data/                        # <--- Put your PDF files in here
â”‚   â”œâ”€ doc1.pdf
â”‚   â”œâ”€ doc2.pdf
â”‚   â””â”€ ...
â”œâ”€ precomputed_data/            # <--- Generated automatically on first run
â”‚   â”œâ”€ index.faiss
â”‚   â”œâ”€ docs.pkl
â”‚   â””â”€ processed_files.pkl
â”œâ”€ templates/
â”‚   â””â”€ index.html               # Flask HTML template for the chat UI
â”œâ”€ requirements.txt             # Python dependencies (see below)
â””â”€ README.md
